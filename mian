import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Создание SparkSession
spark = SparkSession.builder.appName("Partitioning Example").getOrCreate()

# Создание DataFrame
data = [
    (1, "Alice", 34),
    (2, "Bob", 45),
    (3, "Charlie", 29),
    (4, "David", 38),
    (5, "Eve", 31),
    (6, "Frank", 42),
    (7, "Grace", 36),
    (8, "Hannah", 33),
    (9, "Ian", 41),
    (10, "Jack", 37)
]

columns = ["id", "name", "age"]

df = spark.createDataFrame(data, columns)

# Партицирование данных
partitioned_df = df.repartition(3, col("age"))

# Проверка плана выполнения
partitioned_df.explain(True)

# Проверка данных в каждой партиции
def print_partition_id(partition_id, iterator):
    print(f"Partition ID: {partition_id}")
    for record in iterator:
        print(record)

# Использование mapPartitions с collect для вывода данных
partitioned_df.rdd.zipWithIndex().map(lambda x: (x[1] % 3, x[0])).groupByKey().map(lambda x: (x[0], list(x[1]))).collect().foreach(lambda x: print_partition_id(x[0], x[1]))
