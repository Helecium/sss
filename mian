from pyspark.sql import SparkSession
from pyspark.sql.functions import col, spark_partition_id

# Инициализация Spark сессии
spark = SparkSession.builder \
    .appName("Partition Testing") \
    .getOrCreate()

# Создание DataFrame
data = [
    (1, "Alice", 34),
    (2, "Bob", 45),
    (3, "Charlie", 29),
    (4, "David", 38),
    (5, "Eve", 27),
    (6, "Frank", 42),
    (7, "Grace", 33),
    (8, "Hank", 40),
    (9, "Ivy", 31),
    (10, "John", 36)
]

columns = ["id", "name", "age"]

df = spark.createDataFrame(data, columns)

# Проверка количества партиций до переразбиения
print(f"Number of partitions before repartition: {df.rdd.getNumPartitions()}")

# Применение функции repartition
num_partitions = 4
df_repartitioned = df.repartition(num_partitions)

# Проверка количества партиций после переразбиения
print(f"Number of partitions after repartition: {df_repartitioned.rdd.getNumPartitions()}")

# Добавление столбца с идентификатором партиции для проверки
df_repartitioned_with_partition_id = df_repartitioned.withColumn("partition_id", spark_partition_id())

# Вывод данных с идентификатором партиции
df_repartitioned_with_partition_id.show(truncate=False)

# Остановка Spark сессии
spark.stop()
